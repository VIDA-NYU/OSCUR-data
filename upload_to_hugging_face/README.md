# Uploading Datasets to Hugging Face ðŸ¤—

This folder contains scripts and documentation for uploading datasets from this repository to the [Hugging Face Hub](https://huggingface.co/datasets). It supports multiple datasets generated by the project and outlines a reproducible workflow.

---

## Automated Process (Recommended)

To automate the process of uploading a dataset to Hugging Face, note that creating the actual repository on Hugging Face currently requires manual interaction through the web interface. However, once the repository is created, the rest of the upload process can be automated using scripts.

### 1. Create the Repository Manually

Go to [huggingface.co/new](https://huggingface.co/new) and create a new repository:
- Select "Dataset" as the repository type.
- Set the dataset name, description, and visibility (public/private).

### 2. Place Your Dataset

Ensure your dataset is in a folder (e.g., `traffic_data/`) and in one of the following formats:
- CSV, TSV, JSON
- Text files or image folders
- Parquet

### 3. Duplicate and Modify the Template Script

1. Duplicate the `upload_data_template.py` script and rename it appropriately (e.g., `upload_traffic_volume_counts_data.py`).
2. Edit the duplicated script to set the following variables:
   - `local_dataset_path`: Path to the folder containing your dataset (e.g., `./traffic_volume_counts_sample_data`).
   - `csv_filename`: Name of your dataset file (e.g., `sample_traffic.csv`).
   - `repo_id`: Hugging Face dataset repository ID (e.g., `oscur/automated-traffic-volume-counts`).
   - `dataset_title`: Title for the dataset (used in the README).
   - `dataset_description`: Description of the dataset.

### 4. Run the Script

Ensure you have a Hugging Face token. Contact OSCUR members or Sonia Castelo (scq202@nyu.edu) to get the token. Then, run the script you created in Step 3. For example:

```bash
python upload_traffic_volume_counts_data.py
```

Replace `upload_traffic_volume_counts_data.py` with the name of the script you created.

The script will:
1. Automatically generate a `README.md` and `dataset_script.py` for your dataset.
2. Upload the dataset and metadata to the Hugging Face Hub.

Once complete, your dataset will be available at the specified repository URL.

---

## Manual Process

If you prefer to upload datasets manually, follow these steps:

### 1. Prepare Your Dataset

Ensure your dataset is in one of the following formats:
- CSV, TSV, JSON
- Text files or image folders
- Parquet

Organize the data files clearly (e.g., under `data/`), ideally with train/test/dev splits if applicable.

### 2. Install Required Tools

Make sure you have the Hugging Face tools installed:

```bash
pip install huggingface_hub datasets
```

Login to your Hugging Face account:

```bash
huggingface-cli login
```

### 3. Create a Dataset Repository on Hugging Face

Go to https://huggingface.co/new and create a new repository:
- Repository type: Dataset
- Name: your-dataset-name
- Visibility: Public or private

### 4. Clone the Dataset Repository Locally

```bash
git lfs install
git clone https://huggingface.co/datasets/<your-username>/<your-dataset-name>
cd <your-dataset-name>
```

### 5. Add Your Dataset Files

Copy the dataset files into the cloned repository. A suggested structure:

```
your-dataset-name/
â”œâ”€â”€ README.md           # Dataset description
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ test.csv
â”œâ”€â”€ dataset_infos.json  # (Optional) Metadata
â”œâ”€â”€ dataset_script.py   # (Optional) HF loading script
```

Update `README.md` with details such as:
- Dataset description
- Source
- Tasks
- How to use it

### 6. Track and Push Large Files with Git LFS

If you're including large files (e.g., CSVs):

```bash
git lfs track "*.csv"
git add .gitattributes
```

Then commit and push:

```bash
git add .
git commit -m "Add dataset files"
git push
```

### 7. (Optional) Create a Loading Script

If you want your dataset to be directly loadable via:

```python
from datasets import load_dataset
load_dataset("your-username/your-dataset-name")
```

Include a `dataset_script.py` following Hugging Face's custom loader format. This script uses the `datasets` loading API to define `DatasetBuilder`.

<details>
<summary>
   How to Use <code>dataset_script.py</code>
</summary>

>
> - Place this script alongside your data/train.csv, data/test.csv, etc.
> - Push it to your Hugging Face dataset repo.
> - Then test it:
>    ```bash
>     from datasets import load_dataset
>     dataset = load_dataset("your-username/your-dataset-name")
>    ```
</details>

---

## ðŸ“Œ Notes
- The automated process is faster and reduces manual errors.
- Git LFS is required for large files.
- You can update datasets later by re-pushing to the repo.
- Use meaningful and clean structure for discoverability.

Happy uploading! ðŸš€
